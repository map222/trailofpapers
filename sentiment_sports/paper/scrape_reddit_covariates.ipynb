{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# What drives sports player popularity\n",
    "The 2018 Cleveland Cavs made the NBA finals while generating some of the hottest memes of 2018 (\"We got an \\[expletive\\] squad now,\" \"He boomed me.\"). While following discussion of the Cavs on Reddit, I noticed something odd. Turkish rookie Cedi Osman was a particular fan favourite. Cedi played limited minutes with energy, and everyone joked that Cedi was the \"GOAT\" (Greatest Of All Time) carrying Lebron. In contrast, Tristan Thompson, a hero of the 2016 season, had an off year, and was the center of a meme for being traded, \"Shump, TT, and the Nets pick.\" For the Cavs, it commenters seemed to give the white players an easier time. And being a data scientist, I thought, \"I could measure that!\"\n",
    "\n",
    "In this project, we are going to use sentiment and econometric models to try to understand what makes NBA and NFL players popular. I split this project into three parts. Part 1 focuses on scraping player comments from Reddit, and how to download covariate data for players (e.g. performance and demographics). In part 2, I will isolate comments about single players using NER, and calculate sentiment towards those players. In part 3, I will use econometric regression models to investigate what drives player sentiment."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Understanding public sentiment towards players\n",
    "To quantify how the public feels towards player, people in the past have used surveys like the Q-score. While surveys are a great tool for getting precise quantitative answers, they are inherently biased in important ways. First, not everyone responds to surveys, making the sample non-representative; and second, people may not feel like they can be honest in surveys.\n",
    "\n",
    "As an alternative to surveys, we can try to understand how people feel towards players using their natural language. This replaces the survey-response bias with another bias overrepresenting people who speak more often. It also has some advantages: you can get information from a much larger set of people; and people are probably more honest in their appraisals when they don't know they are being measured. As such, I set out to measure the public's sentiment towards players using social media. (Plus, this was all I had access to.)\n",
    "### Scraping comment data from Reddit\n",
    "My primary source of player comments were the active [NBA](https://www.reddit.com/r/nba) and [NFL](https://www.reddit.com/r/nfl) subreddits. I also tried scraping Twitter, but the Twitter API was harder to use.\n",
    "\n",
    "To scrape data, I primarily used the `pushshift` API (I originally used the `praw` module, code for which you can see at the end of this notebook). Since I wanted to scrape data over multiple years and subreddits, I took a note from [Joel Grus](https://www.youtube.com/watch?v=7jiPeIFXb6U), and function-ified my notebook code. For the complete code, see the module `scrape_reddit_comments`.\n",
    "\n",
    "In this section I'm going to go over the main parts of the high level function `get_month_pushshift` from `scrape_reddit_comments`.\n",
    "#### Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "sys.path.append(\"..\")\n",
    "import scrape_reddit_comments as src\n",
    "from scrape_reddit_comments import parse_submission_pushshift"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import time\n",
    "import json\n",
    "import requests\n",
    "from datetime import datetime\n",
    "pd.set_option('display.max_colwidth', 100)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Scraping code\n",
    "#### Initialize variables\n",
    "Before getting into the code proper, I am going to initialize some variables."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [],
   "source": [
    "# year and month are parameters that need to be defined\n",
    "year, month, day = 2018, 1, 31\n",
    "data_col = ['text', 'timestamp', 'user', 'flair', 'score', 'id', 'link_id', 'parent_id']\n",
    "start_after = datetime.now() - datetime(year,month,1)\n",
    "end_before = datetime.now() - datetime(year,month,day)\n",
    "start_hour = start_after.days * 24 + start_after.seconds  // 3600\n",
    "end_hour = end_before.days * 24 + end_before.seconds  // 3600 - 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Setting up the API query\n",
    "Pushshift uses a RESTful HTTP API, so we need to set up the URL and parameter information ahead of time. This example is for original posts (\"submissions\"); you can check the module for the comment URL."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "url_params = {'subreddit': 'nba',\n",
    "              'size':500}\n",
    "submission_url = 'https://api.pushshift.io/reddit/search/submission/'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Running the query for a month\n",
    "I use `requests` to do the http query. To get a whole month of data, I repeat the query with six hour increments. I also include a `time.sleep` to prevent the API from getting overloaded."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Downloading submissions for 2018-1\n"
     ]
    }
   ],
   "source": [
    "# initialize list of submissions\n",
    "month_submissions = []\n",
    "\n",
    "# run data request for \"submissions\" (original post)\n",
    "print('Downloading submissions for {}-{}'.format(year, month))\n",
    "hour_step = 6\n",
    "for hour in range(start_hour, end_hour, -hour_step):\n",
    "    url_params.update({'before': str(hour)+'h', 'after': str(hour+hour_step) + 'h'})\n",
    "    month_submissions.extend(json.loads(requests.get(submission_url, params=url_params).text)['data'])\n",
    "    time.sleep(0.5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Parsing the submissions\n",
    "Pushshift returns a JSON object for each query. To parse these, I created a helper function `parse_submission_pushshift`. Then I convert it into a dataframe."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Made dataframe of shape (23011, 9)\n"
     ]
    }
   ],
   "source": [
    "ops = [ parse_submission_pushshift(submission) for submission in month_submissions]\n",
    "submission_df =(pd.DataFrame(ops, columns=data_col)\n",
    "                  .assign(source = lambda x: 'submission') )\n",
    "print('Made dataframe of shape {}'.format(submission_df.shape) )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The returned dataframe contains the text, timestamp, and other metadata from the original post."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>text</th>\n",
       "      <th>timestamp</th>\n",
       "      <th>user</th>\n",
       "      <th>flair</th>\n",
       "      <th>score</th>\n",
       "      <th>id</th>\n",
       "      <th>link_id</th>\n",
       "      <th>parent_id</th>\n",
       "      <th>source</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>GET MANU GINOBILI TO BE A WEST CAPTAIN. [removed]</td>\n",
       "      <td>1514775573</td>\n",
       "      <td>manuginobilistan</td>\n",
       "      <td>None</td>\n",
       "      <td>1</td>\n",
       "      <td>7ncuar</td>\n",
       "      <td>7ncuar</td>\n",
       "      <td>7ncuar</td>\n",
       "      <td>submission</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>PJ Tucker picks up the offensive layup then lays it up and in, with 5 seconds remaining in 2OT. ...</td>\n",
       "      <td>1514775669</td>\n",
       "      <td>[deleted]</td>\n",
       "      <td>None</td>\n",
       "      <td>3</td>\n",
       "      <td>7ncukx</td>\n",
       "      <td>7ncukx</td>\n",
       "      <td>7ncukx</td>\n",
       "      <td>submission</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                                                                  text  \\\n",
       "0                                                    GET MANU GINOBILI TO BE A WEST CAPTAIN. [removed]   \n",
       "1  PJ Tucker picks up the offensive layup then lays it up and in, with 5 seconds remaining in 2OT. ...   \n",
       "\n",
       "    timestamp              user flair  score      id link_id parent_id  \\\n",
       "0  1514775573  manuginobilistan  None      1  7ncuar  7ncuar    7ncuar   \n",
       "1  1514775669         [deleted]  None      3  7ncukx  7ncukx    7ncukx   \n",
       "\n",
       "       source  \n",
       "0  submission  \n",
       "1  submission  "
      ]
     },
     "execution_count": 56,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "submission_df.head(2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "I then repeat the above process for the comments, using a smaller time step (one hour), and a different helper function (`parse_comment_pushshift`).\n",
    "### Scraping data using the module\n",
    "Once you understand the basic code for scraping, you can do things more easily using the `get_month_pushshift` function. The function takes four parameters:\n",
    "* year: integer of year you want to scrape (I scraped back to ~2013)\n",
    "* month: integer of month you want to scrape\n",
    "* last_day: integer of last day of month; I manually changed this for every month; you should probably use a lookup table!\n",
    "* league: str name of subreddit you want to scrape. I used `nfl` and `nba`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "league = 'nba'\n",
    "year = 2015\n",
    "month = 12\n",
    "month_df, submissions, comments = src.get_month_pushshift(year,month,30, league)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Running the above code usually takes 20-25 minutes. Pushshift typically yields ~300k comments / month for a month during the season."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>text</th>\n",
       "      <th>timestamp</th>\n",
       "      <th>user</th>\n",
       "      <th>flair</th>\n",
       "      <th>score</th>\n",
       "      <th>id</th>\n",
       "      <th>link_id</th>\n",
       "      <th>parent_id</th>\n",
       "      <th>source</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>99440</th>\n",
       "      <td>MARK JACKSON FOR GM! #thenightmare</td>\n",
       "      <td>1.452452e+09</td>\n",
       "      <td>Asprobouboulis</td>\n",
       "      <td>Spurs</td>\n",
       "      <td>8.0</td>\n",
       "      <td>cyt3rc8</td>\n",
       "      <td>t3_40cist</td>\n",
       "      <td>t3_40cist</td>\n",
       "      <td>comment</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>91900</th>\n",
       "      <td>I don't think that anybody is going to try to argue that Lebron is a better scorer than MJ. Care...</td>\n",
       "      <td>1.452380e+09</td>\n",
       "      <td>BIGJ0N</td>\n",
       "      <td>Charlotte Hornets</td>\n",
       "      <td>5.0</td>\n",
       "      <td>cys8p4l</td>\n",
       "      <td>t3_40728l</td>\n",
       "      <td>t1_cys0qgl</td>\n",
       "      <td>comment</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                                                                      text  \\\n",
       "99440                                                                   MARK JACKSON FOR GM! #thenightmare   \n",
       "91900  I don't think that anybody is going to try to argue that Lebron is a better scorer than MJ. Care...   \n",
       "\n",
       "          timestamp            user              flair  score       id  \\\n",
       "99440  1.452452e+09  Asprobouboulis              Spurs    8.0  cyt3rc8   \n",
       "91900  1.452380e+09          BIGJ0N  Charlotte Hornets    5.0  cys8p4l   \n",
       "\n",
       "         link_id   parent_id   source  \n",
       "99440  t3_40cist   t3_40cist  comment  \n",
       "91900  t3_40728l  t1_cys0qgl  comment  "
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "month_df.sample(2, random_state=24607)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Save / load\n",
    "Finally, after downloading the comments, I save them to disk. Note the use of f-strings, which allow you to use the league, year, and month parameters to name your file. (Something weird is going on where is throws a `UnicodeDecodeError` when writing to a gzipped file.)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "month_df.to_csv(f'd:/data/sentiment_sports/{league}_reddit_comments/{year}{month:02}-comments_submissions.tsv',\n",
    "                sep = '\\t', encoding = 'utf-8',index = False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Scraping covariate data\n",
    "In addition to scraping the player comments, I needed to scrape demographic and performance data in order to understand what drives player sentiment. To do this, I built a set of functions that queried [basketball-reference.com](https://basketball-reference.com), [pro-football-reference.com](https://pro-football-reference.com), [footballoutsiders.com](https://footballoutsiders.com/), and Wikipedia. In this section I won't go over the specific code for each dataset, but just show how to query the data.\n",
    "#### Import module"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [],
   "source": [
    "import scrape_player_data as spd"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### NBA\n",
    "#### Scrape player performance\n",
    "I wrote a function to scrape NBA player info from `basketball-reference.com`, then simply concatenate the results into a single dataframe."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "performance2018_df = spd.get_year_performance_nba(2018).assign(season = lambda row: row['year'] -1)\n",
    "performance2017_df = spd.get_year_performance_nba(2017).assign(season = lambda row: row['year'] -1)\n",
    "performance2016_df = spd.get_year_performance_nba(2016).assign(season = lambda row: row['year'] -1)\n",
    "performance2015_df = spd.get_year_performance_nba(2015).assign(season = lambda row: row['year'] -1)\n",
    "performance2014_df = spd.get_year_performance_nba(2014).assign(season = lambda row: row['year'] -1)\n",
    "performance2013_df = spd.get_year_performance_nba(2013).assign(season = lambda row: row['year'] -1)\n",
    "performance_df = pd.concat([performance2013_df, performance2014_df, performance2015_df, performance2016_df, performance2017_df, performance2018_df])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Remember to save your data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "performance_df.to_csv('nba_performance.tsv', sep='\\t', index = False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Player demographics\n",
    "It was difficult to find a list of NBA player ethnicities, so I just googled the pictures of a few hundred players. I downloaded height and weight information from basketball-reference.com."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "demo_df = pd.read_csv('d:/data/sentiment_sports/covariates/nba_players2013.tsv', sep ='\\t', encoding = 'utf-8')\n",
    "demo_df['Player'] = demo_df['Player'].str.lower()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "While parsing the heights, it sometimes gets saved as a date. Convert these dates to inches."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "demo_df['Ht'] = demo_df['Ht'].str.split('-')\n",
    "height_dict = {'Jun':72, 'Jul':84, 'May':60}\n",
    "height_dict.update({str(x):x for x in range(13)})\n",
    "height_dict.update({'00':0})\n",
    "demo_df['Ht'] = demo_df['Ht'].apply(lambda row: height_dict[row[0]]*12 + height_dict[row[1]])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Team info\n",
    "I downloaded team performance data from basketball-reference.com using `pandas.read_html`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for year in range(2014, 2019):\n",
    "    team_year_df = pd.concat(pd.read_html(f'https://www.basketball-reference.com/leagues/NBA_{year}.html')[:2]).assign(year = year)\n",
    "    team_year_df.to_csv(f'd:/data/sentiment_sports/covariates/nba_teams{year}.tsv', sep='\\t', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# then hand edit the 3-letter team names\n",
    "team_df = pd.concat([pd.read_csv('d:/data/sentiment_sports/covariates/nba_teams2014.tsv', sep='\\t'),\n",
    "                     pd.read_csv('d:/data/sentiment_sports/covariates/nba_teams2015.tsv', sep='\\t'),\n",
    "                     pd.read_csv('d:/data/sentiment_sports/covariates/nba_teams2016.tsv', sep='\\t'),\n",
    "                     pd.read_csv('d:/data/sentiment_sports/covariates/nba_teams2017.tsv', sep='\\t'),\n",
    "                     pd.read_csv('d:/data/sentiment_sports/covariates/nba_teams2018.tsv', sep='\\t')])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Salary\n",
    "I downloaded salary information from [HoopsHype](https://hoopshype.com/salaries/) using `pandas.read_html`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\ProgramData\\Anaconda3\\lib\\site-packages\\ipykernel_launcher.py:5: FutureWarning: Sorting because non-concatenation axis is not aligned. A future version\n",
      "of pandas will change to not sort by default.\n",
      "\n",
      "To accept the future behavior, pass 'sort=True'.\n",
      "\n",
      "To retain the current behavior and silence the warning, pass sort=False\n",
      "\n",
      "  \"\"\"\n"
     ]
    }
   ],
   "source": [
    "def get_nba_salary_year(season):\n",
    "    return (pd.read_html(f'https://hoopshype.com/salaries/players/{season}-{season+1}/')[0]\n",
    "              .rename(columns = {str(season) + '/' +  str(season+1)[2:]: 'salary'})\n",
    "              .assign(season = season))\n",
    "salary_df = pd.concat([get_nba_salary_year(season) for season in range(2010, 2019)])[['Player', 'salary', 'season']]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The salary format was a little wonky, so I used `locale` to fix it"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "salary_df['Player'] = salary_df['Player'].str.lower()\n",
    "import locale\n",
    "locale.setlocale( locale.LC_ALL, 'en_US.UTF-8' ) \n",
    "salary_df['salary'] = salary_df['salary'].str[1:].apply(locale.atoi)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Salary standardization\n",
    "For regression, I also tried to standardize the salaries.  After taking the 4th root, things looked decent."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "salary_df['standard_salary'] = np.power(salary_df['salary'], 1/4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "np.power(salary_df['salary'], 1/4).hist();"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### City information\n",
    "When thinking about race, we considered whether the demographics of the city influence player popularity. I downloaded census and polling data for both NBA and NFL cities (I forgot the sites). I then spent some time making sure all the join keys were correct.\n",
    "#### City demographics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "# city demographics\n",
    "city_df = pd.read_csv('d:/data/sentiment_sports/covariates/sports_metro_demographics.csv').drop(columns = ['census_Id', 'census_Id2', 'Geography'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### City voting during 2016 election"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "vote_df = pd.read_excel('d:/data/sentiment_sports/covariates/2016 election results by county.xlsx')\n",
    "vote_df['county_name'] = vote_df['county_name'].str.lower().str.replace(' county', '')"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "msa_df = pd.read_excel('data/covariates/County to MSA Map.xlsx')\n",
    "msa_df['County'] = msa_df['County'].str.lower()\n",
    "msa_df = msa_df.rename(columns = {'County':'county_name',\n",
    "                                  'State':'state_abbr'})"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "vote_df = vote_df.merge(msa_df, on= ['county_name', 'state_abbr']).drop_duplicates(['state_abbr', 'county_name'])\n",
    "vote_df.sort_values('total_votes', ascending=False).to_csv('data/covariates/msa_votes.tsv', sep='\\t')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "vote_df = pd.read_csv('d:/data/sentiment_sports/covariates/msa_votes.tsv', sep='\\t')[['per_point_diff', 'Tm']].rename(columns={'per_point_diff':'clinton_vote_lead'})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Combine info\n",
    "After getting all of the data for various covariates, I combined them into a single dataframe that has performance, demographics, and city information."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Player</th>\n",
       "      <th>Race</th>\n",
       "      <th>From</th>\n",
       "      <th>To</th>\n",
       "      <th>Pos_x</th>\n",
       "      <th>Ht</th>\n",
       "      <th>Wt</th>\n",
       "      <th>Birth Date</th>\n",
       "      <th>Colleges</th>\n",
       "      <th>Rk</th>\n",
       "      <th>...</th>\n",
       "      <th>year</th>\n",
       "      <th>season</th>\n",
       "      <th>Wins</th>\n",
       "      <th>total_population</th>\n",
       "      <th>metro_percent_white</th>\n",
       "      <th>metro_percent_black</th>\n",
       "      <th>clinton_vote_lead</th>\n",
       "      <th>salary</th>\n",
       "      <th>standard_salary</th>\n",
       "      <th>experience</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>alex abrines</td>\n",
       "      <td>W</td>\n",
       "      <td>2017</td>\n",
       "      <td>2018</td>\n",
       "      <td>G-F</td>\n",
       "      <td>78</td>\n",
       "      <td>190</td>\n",
       "      <td>August 1, 1993</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1</td>\n",
       "      <td>...</td>\n",
       "      <td>2017</td>\n",
       "      <td>2016</td>\n",
       "      <td>47</td>\n",
       "      <td>1337075</td>\n",
       "      <td>74.1</td>\n",
       "      <td>10.2</td>\n",
       "      <td>0.1051</td>\n",
       "      <td>5994764.0</td>\n",
       "      <td>49.481519</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>steven adams</td>\n",
       "      <td>W</td>\n",
       "      <td>2014</td>\n",
       "      <td>2018</td>\n",
       "      <td>C</td>\n",
       "      <td>84</td>\n",
       "      <td>255</td>\n",
       "      <td>July 20, 1993</td>\n",
       "      <td>University of Pittsburgh</td>\n",
       "      <td>3</td>\n",
       "      <td>...</td>\n",
       "      <td>2017</td>\n",
       "      <td>2016</td>\n",
       "      <td>47</td>\n",
       "      <td>1337075</td>\n",
       "      <td>74.1</td>\n",
       "      <td>10.2</td>\n",
       "      <td>0.1051</td>\n",
       "      <td>3140517.0</td>\n",
       "      <td>42.096917</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>2 rows Ã— 69 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "         Player Race  From    To Pos_x  Ht   Wt      Birth Date  \\\n",
       "0  alex abrines    W  2017  2018   G-F  78  190  August 1, 1993   \n",
       "1  steven adams    W  2014  2018     C  84  255   July 20, 1993   \n",
       "\n",
       "                   Colleges  Rk     ...      year  season  Wins  \\\n",
       "0                       NaN   1     ...      2017    2016    47   \n",
       "1  University of Pittsburgh   3     ...      2017    2016    47   \n",
       "\n",
       "   total_population  metro_percent_white  metro_percent_black  \\\n",
       "0           1337075                 74.1                 10.2   \n",
       "1           1337075                 74.1                 10.2   \n",
       "\n",
       "   clinton_vote_lead     salary  standard_salary  experience  \n",
       "0             0.1051  5994764.0        49.481519           0  \n",
       "1             0.1051  3140517.0        42.096917           3  \n",
       "\n",
       "[2 rows x 69 columns]"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "covariates_df = (demo_df.merge(performance_df, on = 'Player')\n",
    "                      .merge(team_df, on=['Tm', 'year'])\n",
    "                      .merge(city_df, on='Tm')\n",
    "                      .merge(vote_df, on='Tm') )\n",
    "covariates_df['Player'] = covariates_df['Player'].str.replace('.', '')\n",
    "covariates_df = covariates_df.merge(salary_df, on=['Player', 'season'], how='left')\n",
    "covariates_df['experience'] = covariates_df['year'] - covariates_df['From']\n",
    "covariates_df.head(2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Coach data\n",
    "In addition to getting player data, I wanted to get coach data. For coaches, we have fewer features (just wins). Here, I wrote a simple function to scrape basketball-reference.com, `scrape_nba_coaches`. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "importlib.reload(spd)\n",
    "nba_coach_df = spd.scrape_nba_coaches()\n",
    "nba_coach_df.to_csv('d:/data/sentiment_sports/covariates/nba_coach_performance.tsv', sep='\\t', index=False)"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "# then hand edit it to assign race\n",
    "nba_coach_df.query('season >= 2013')[['Coach']].drop_duplicates().to_csv('d:/data/sentiment_sports/covariates/nba_coach_race.tsv', sep='\\t', index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## NFL data\n",
    "In addition to understanding what drives NBA player sentiment, I wanted to look at other sports, specifically the NFL. Rather than step through each individual function to scrape the data, I would instead refer you to the `scrape_player_data` module."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data for download\n",
    "As the covariate data is rather small, and all publicly sourced, I have uploaded it to the github repo for this project: `map222/trailofpapers/sentiment_spors/modeling_data`."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Appendiex\n",
    "### Use PRAW and reddit API to get lots of comments\n",
    "Before using pushshift, I used the actual reddit API. Unfortunately, an API update broke this code. If you figure out how to run this, let me know!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "from datetime import datetime\n",
    "import json\n",
    "import pandas as pd\n",
    "import praw # for direct reddit pull"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "client_id = 'nope'\n",
    "secret = 'nope'\n",
    "user_agent = 'r/nba race sentiment 0.1 by /u/Umiy'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "r = praw.Reddit(user_agent=user_agent, client_id=client_id, client_secret=secret)\n",
    "r_nba = r.subreddit('nba')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 138,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def parse_submission( submission):\n",
    "    text = submission.title + '. ' + submission.selftext\n",
    "    creation_date = submission.created\n",
    "    author = submission.author #.name for PRAW\n",
    "    flair = submission.author_flair_text\n",
    "    score = submission.score\n",
    "    return (text, creation_date, author, flair, score)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 140,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def parse_comment( comment):\n",
    "    if hasattr(comment, 'body') and comment.author != None:\n",
    "        text = comment.body\n",
    "        creation_date = comment.created\n",
    "        author = comment.author\n",
    "        flair = comment.author_flair_text\n",
    "        score = comment.score\n",
    "        return (text, creation_date, author, flair, score)\n",
    "    return ('', 1, '', '', 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def get_month(year, month, month_length):\n",
    "    ''' Get comments and submissions for one month. takes ~ 1h 15 minutes.\n",
    "    '''\n",
    "    data_col = ['text', 'timestamp', 'user', 'flair']\n",
    "    print('Downloading submssions for {}-{}'.format(year, month))\n",
    "    month_submissions = [list(r_nba.submissions(datetime(year,month,day).timestamp(), datetime(year,month,day+1).timestamp() )) for day in range(1,month_length)]\n",
    "    month_submissions = [x for day_submissions in month_submissions for x in day_submissions]\n",
    "    \n",
    "    print('Downloaded {} submissions'.format(len(month_submissions)))\n",
    "    ops = [ parse_submission(submission) for submission in month_submissions]\n",
    "    submission_df =(pd.DataFrame(ops, columns=data_col)\n",
    "                      .assign(source = lambda x: 'submission') )\n",
    "    print('Made dataframe of shape {}'.format(submission_df.shape) )\n",
    "    \n",
    "    print('Downloading comments (this could take an hour)')\n",
    "    comments_list = [submission.comments.list() for submission in month_submissions if hasattr(submission, 'comments')]\n",
    "    comments = [ parse_comment(comment) for comments in  comments_list for comment in comments]\n",
    "    print('Downloaded {} comments'.format(len(comments) ) )\n",
    "    comment_df = (pd.DataFrame(comments, columns=data_col)\n",
    "                    .assign(source = lambda x: 'comment') )\n",
    "    return pd.concat([submission_df, comment_df]), ops, comments"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "# this no longer works \n",
    "month_df, submissions, comments = get_month(2017, 11, 30)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
